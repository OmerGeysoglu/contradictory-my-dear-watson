{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"},{"sourceId":177200,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":150943,"modelId":173416}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport transformers\nfrom transformers import AutoModelForSequenceClassification,XLMRobertaTokenizer, get_scheduler\nfrom datasets import load_dataset\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ntransformers.logging.set_verbosity_error()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:42:21.245235Z","iopub.execute_input":"2024-11-24T23:42:21.245991Z","iopub.status.idle":"2024-11-24T23:42:24.500841Z","shell.execute_reply.started":"2024-11-24T23:42:21.245954Z","shell.execute_reply":"2024-11-24T23:42:24.500143Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class CFG:\n    batch_size = 64\n    sequence_length = 128\n    epochs = 4\n    learning_rate = 1e-5\n    weight_decay = 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:42:24.502124Z","iopub.execute_input":"2024-11-24T23:42:24.502590Z","iopub.status.idle":"2024-11-24T23:42:24.507089Z","shell.execute_reply.started":"2024-11-24T23:42:24.502552Z","shell.execute_reply":"2024-11-24T23:42:24.506146Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest_data = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntrain_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:42:24.957599Z","iopub.execute_input":"2024-11-24T23:42:24.958137Z","iopub.status.idle":"2024-11-24T23:42:25.057232Z","shell.execute_reply.started":"2024-11-24T23:42:24.958106Z","shell.execute_reply":"2024-11-24T23:42:25.056424Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           id                                            premise  \\\n0  5130fd2cb5  and these comments were considered in formulat...   \n1  5b72532a0b  These are issues that we wrestle with in pract...   \n2  3931fbe82a  Des petites choses comme celles-là font une di...   \n3  5622f0c60b  you know they can't really defend themselves l...   \n4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n\n                                          hypothesis lang_abv language  label  \n0  The rules developed in the interim were put to...       en  English      0  \n1  Practice groups are not permitted to work on t...       en  English      2  \n2              J'essayais d'accomplir quelque chose.       fr   French      0  \n3  They can't defend themselves because of their ...       en  English      0  \n4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lang_abv</th>\n      <th>language</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5130fd2cb5</td>\n      <td>and these comments were considered in formulat...</td>\n      <td>The rules developed in the interim were put to...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5b72532a0b</td>\n      <td>These are issues that we wrestle with in pract...</td>\n      <td>Practice groups are not permitted to work on t...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3931fbe82a</td>\n      <td>Des petites choses comme celles-là font une di...</td>\n      <td>J'essayais d'accomplir quelque chose.</td>\n      <td>fr</td>\n      <td>French</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5622f0c60b</td>\n      <td>you know they can't really defend themselves l...</td>\n      <td>They can't defend themselves because of their ...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>86aaa48b45</td>\n      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n      <td>th</td>\n      <td>Thai</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model_path = 'joeddav/xlm-roberta-large-xnli'\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:42:28.031489Z","iopub.execute_input":"2024-11-24T23:42:28.031862Z","iopub.status.idle":"2024-11-24T23:42:30.243215Z","shell.execute_reply.started":"2024-11-24T23:42:28.031829Z","shell.execute_reply":"2024-11-24T23:42:30.242255Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#PATH = '/kaggle/input/xlm-roberta-large-xnli-fine-tuned/pytorch/default/1/BERT_ft_epochmodel.model'\n#model.load_state_dict(torch.load(PATH, weights_only=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:42:40.239132Z","iopub.execute_input":"2024-11-24T23:42:40.239697Z","iopub.status.idle":"2024-11-24T23:42:40.243660Z","shell.execute_reply.started":"2024-11-24T23:42:40.239662Z","shell.execute_reply":"2024-11-24T23:42:40.242754Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ContradictoryDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        self.df = df\n        self.tokens = tokenizer(\n            df['premise'].tolist(),\n            df['hypothesis'].tolist(),\n            max_length = CFG.sequence_length,\n            truncation=True,\n            padding='max_length',\n            add_special_tokens=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        if 'label' in df.keys():\n          self.labels = torch.tensor(df['label'].tolist())\n\n\n    def __len__(self):\n        return self.tokens[\"input_ids\"].shape[0]\n\n    def __getitem__(self, idx):\n      if 'label' in self.df.keys():\n        return (\n            self.tokens[\"input_ids\"][idx], \n            self.tokens[\"attention_mask\"][idx],\n            self.labels[idx]\n        )\n      return (\n            self.tokens[\"input_ids\"][idx], \n            self.tokens[\"attention_mask\"][idx],\n        )\n\n# https://huggingface.co/datasets/nyu-mll/multi_nli\ndef load_mnli_dataset():\n  mnli_dataset = load_dataset('multi_nli')\n  result = []\n\n  for data in mnli_dataset['train']:\n    x1, x2, y = data['premise'], data['hypothesis'], data['label']\n    if x1 and x2 and y in {0, 1, 2}:\n      result.append((x1, x2, y, 'en'))\n  df = pd.DataFrame(result, columns=['premise', 'hypothesis', 'label', 'lang_abv'])\n  return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:42:40.505046Z","iopub.execute_input":"2024-11-24T23:42:40.505778Z","iopub.status.idle":"2024-11-24T23:42:40.513586Z","shell.execute_reply.started":"2024-11-24T23:42:40.505749Z","shell.execute_reply":"2024-11-24T23:42:40.512550Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# loading additional multi-genre NLI dataset\nmnli_dataset = load_mnli_dataset()\n\n# train/validation/test split\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\ntrain_data = pd.concat([train_data, mnli_dataset.iloc[:100000]], axis=0)\n\n# creating datasets\ntrain_dataset = ContradictoryDataset(train_data, tokenizer)\nval_dataset = ContradictoryDataset(val_data, tokenizer)\ntest_dataset = ContradictoryDataset(test_data, tokenizer)\n\n# creating dataloaders\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=CFG.batch_size,\n    num_workers=0\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=CFG.batch_size,\n    num_workers=0\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=CFG.batch_size,\n    num_workers=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:42:41.095154Z","iopub.execute_input":"2024-11-24T23:42:41.095910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, train_dataloader, criterion, optimizer, epoch):\n    train_loss = 0.0\n    train_f1 = 0.0\n    train_progress_bar = tqdm(train_dataloader, desc=\"Epoch {:1d}\".format(epoch), leave=False, disable=False)\n    \n    for i, batch in enumerate(train_progress_bar):\n      optimizer.zero_grad()\n      batch = tuple(b.to(device) for b in batch)\n    \n      inputs = {\n          'input_ids': batch[0],\n          'attention_mask': batch[1],\n          'labels': batch[2]\n      }\n          \n      outputs = model(**inputs)\n      loss = outputs[0]\n      logits = outputs[1]\n      \n    \n      logits = logits.detach().cpu().numpy()\n      label_ids = inputs['labels'].cpu().numpy()\n    \n      y_pred = np.argmax(logits, axis=1)\n      y_true = label_ids\n        \n      train_loss += loss.item()\n      train_f1 += f1_score(y_true, y_pred, average='weighted')\n    \n      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n      loss.backward()\n      optimizer.step()\n        \n      train_progress_bar.set_postfix({'train_loss': f'{(train_loss / (i + 1)):.4f}', 'train_f1': f'{(train_f1 / (i + 1)):.4f}'})\n\n    return train_loss / len(train_dataloader), train_f1 / len(train_dataloader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(model, val_dataloader, criterion, epoch):\n    val_loss = 0.0\n    val_f1 = 0.0\n    val_progress_bar = tqdm(val_dataloader, desc=\"Epoch {:1d}\".format(epoch), leave=False, disable=False)\n\n    for i, batch in enumerate(val_progress_bar):\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {\n          'input_ids': batch[0],\n          'attention_mask': batch[1],\n          'labels': batch[2]\n          }\n\n        with torch.no_grad():\n          outputs = model(**inputs)\n        loss = outputs[0]\n        logits = outputs[1]\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n\n        y_pred = np.argmax(logits, axis=1).flatten()\n        y_true = label_ids\n\n        val_loss += loss.item()\n        val_f1 += f1_score(y_true, y_pred, average='weighted')\n    \n        val_progress_bar.set_postfix({'val_loss': f'{(val_loss / (i + 1)):.4f}', 'val_f1': f'{(val_f1 / (i + 1)):.4f}'})\n\n    return val_loss / len(val_dataloader), val_f1 / len(val_dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(model, test_dataloader):\n    model.eval()\n    y_pred = []\n    test_progress_bar = tqdm(test_dataloader, desc=\"Test\", leave=False, disable=False)\n\n    for batch in test_progress_bar:\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {\n          'input_ids': batch[0],\n          'attention_mask': batch[1],\n          }\n\n        with torch.no_grad():\n          outputs = model(**inputs)\n        logits = outputs[0]        \n        logits = logits.detach().cpu().numpy()\n        y_pred.extend(np.argmax(logits, axis=1)) \n\n    return y_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, train_dataloader, criterion, optimizer, epochs):\n    model.train()\n    train_losses = []\n    val_losses = []\n\n    train_scores = []\n    val_scores = []\n    \n    for epoch in range(epochs):\n      # training mode\n      model.train()\n      train_loss, train_f1 = train_one_epoch(model, train_dataloader, criterion, optimizer, epoch+1)\n      train_losses.append(train_loss)\n      train_scores.append(train_f1)\n\n      # validation mode\n      model.eval()\n      val_loss, val_f1 = validate(model, val_dataloader, criterion, epoch+1)\n      val_losses.append(val_loss)\n      val_scores.append(val_f1)\n\n      # saving model checkpoints in each epoch\n      torch.save(model.state_dict(), 'BERT_MNLI_epoch{}.model'.format(epoch+1))\n\n    return {\n        'train_loss': train_losses,\n        'val_loss': val_losses,\n        'train_score': train_scores,\n        'val_score': val_scores,\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training the model and getting the history of loss and f1 score\nhistory = train(model, train_dataloader, criterion, optimizer, CFG.epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# getting test predictions\ny_preds = test(model, test_dataloader)\nsubmission = test_data.id.copy().to_frame()\nsubmission['prediction'] = y_preds\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = test_data.id.copy().to_frame()\nsubmission['prediction'] = preds\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}